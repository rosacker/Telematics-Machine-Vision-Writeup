{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Project Goal \u00b6 The idea behind this project is that a large amount of contextual information is available from the driver's perspective when operating a vehicle. The driver can see all the other entities operating on the road, they can see any stop lights or road signs that affect how they should drive, they can see light/weather conditions that should change how cautiously they drive, and really many more. The main sense humans use when they drive is their vision, so theoretically almost everything the human is contemplating when operating a vehicle should be observable through a machine vision solution. This project looks to demonstrate ways that a camera could supplement traditional insurance telematics devices in order to capture additional contextual information about an individual's driving habits. If such a solution were to be deployed, the information captured by the camera would be fed as features into a downstream supervised learning model which predicts long term likelihood of incurring losses on an insurance policy.","title":"Project Goal"},{"location":"#project_goal","text":"The idea behind this project is that a large amount of contextual information is available from the driver's perspective when operating a vehicle. The driver can see all the other entities operating on the road, they can see any stop lights or road signs that affect how they should drive, they can see light/weather conditions that should change how cautiously they drive, and really many more. The main sense humans use when they drive is their vision, so theoretically almost everything the human is contemplating when operating a vehicle should be observable through a machine vision solution. This project looks to demonstrate ways that a camera could supplement traditional insurance telematics devices in order to capture additional contextual information about an individual's driving habits. If such a solution were to be deployed, the information captured by the camera would be fed as features into a downstream supervised learning model which predicts long term likelihood of incurring losses on an insurance policy.","title":"Project Goal"},{"location":"Dataset/","text":"Dataset \u00b6 The dataset selected for this project comes from \"BDD100K: A Large-scale Diverse Driving Video Database\". Much more detail on this dataset can be found on their website . The dataset includes 100,000 images from a frontward facing camera with a wide variety of modeling tasks. This includes tasks from the domains of image classification, object detection and image segmentation. This dataset was selected due to the sheer number of images available, the high degree of variety in the images, and the fact that one single dataset could be used for many different applicable tasks. Below is a small sample of the types of images/labels available in the dataset 1 : Detection Vehicle Segmentation Area Segmentation Driveable Area Pose Additional Dataset History \u00b6 The dataset was originally published in 2018 2 with a total of 10 tasks available for future authors to test their machine vision models on. For the purposes of this project, an enhanced set of labels were used which were released in a follow project in 2020. Dataset Bias Consideration \u00b6 As shown below, images for the dataset were collected in major US cities. While capturing the data in these location certainly works well for building a challenging dataset with many targets, it does present a bias when you look to apply the model in other portions of the US. A review of the images did not find many photos that were reflective of suburban or rural geographic locations, and models trained on this dataset are seemingly not likely to generalize well to those areas. Image Source - https://www.bdd100k.com \u21a9 Source - https://arxiv.org/abs/1805.04687 \u21a9","title":"Dataset"},{"location":"Dataset/#dataset","text":"The dataset selected for this project comes from \"BDD100K: A Large-scale Diverse Driving Video Database\". Much more detail on this dataset can be found on their website . The dataset includes 100,000 images from a frontward facing camera with a wide variety of modeling tasks. This includes tasks from the domains of image classification, object detection and image segmentation. This dataset was selected due to the sheer number of images available, the high degree of variety in the images, and the fact that one single dataset could be used for many different applicable tasks. Below is a small sample of the types of images/labels available in the dataset 1 : Detection Vehicle Segmentation Area Segmentation Driveable Area Pose","title":"Dataset"},{"location":"Dataset/#additional_dataset_history","text":"The dataset was originally published in 2018 2 with a total of 10 tasks available for future authors to test their machine vision models on. For the purposes of this project, an enhanced set of labels were used which were released in a follow project in 2020.","title":"Additional Dataset History"},{"location":"Dataset/#dataset_bias_consideration","text":"As shown below, images for the dataset were collected in major US cities. While capturing the data in these location certainly works well for building a challenging dataset with many targets, it does present a bias when you look to apply the model in other portions of the US. A review of the images did not find many photos that were reflective of suburban or rural geographic locations, and models trained on this dataset are seemingly not likely to generalize well to those areas. Image Source - https://www.bdd100k.com \u21a9 Source - https://arxiv.org/abs/1805.04687 \u21a9","title":"Dataset Bias Consideration"},{"location":"Deployment/","text":"Architecture \u00b6 The majority of this solution was deployed using AWS Sagemaker and AWS Rekognition along with standard AWS services such as Lambda, Step Functions, DynamoDB. Synchronous Trip Data Capture \u00b6 The first half of this system focuses on the capture of driver data. A small website was developed ( source code ) that a user can access with their Smartphone. When the user starts a \"trip\", images and GPS data are automatically captured every 5 seconds. This data is sent to an API which saves the photo and associated metadata for future processing. Visualized below are a series of photos captured through this system at 5 second intervals and 480p resolution. Asynchronous Trip Summary Pipeline \u00b6 As a batch process, trip data is processed through a machine vision pipeline. When triggered, the batch process hosts the 3 sagemaker models discussed in the \"Models\" section of this book on Sagemaker Endpoints. The pipeline then loops through each unique trip id, and grabs all of the associated image URLs from a DynamoDB database. Each image is then processed through a Lambda which invokes the Sagemaker Endpoints as well as the Rekognition detect_labels API. The model detections are then stored in another DynamoDB database for downstream use. Finally, the sagemaker endpoints are shut down once all trips have been processed. The backend source code can be found here Future Enhancements \u00b6 Scale \u00b6 The system built for this project works well for small-medium scale. However, for a major insurance company to use it, there would likely need to be some hardening done on the batch pipeline. One service to look into would be AWS Kinesis which may provide better scalability when processing potentially millions of images daily. Security \u00b6 The initial system does not have user identity or user authentication implemented. Implementing a service such as AWS Cognito would be needed in order to make this production grade. User Data Limits \u00b6 This system sends one 480p image every 5 seconds from a user's smartphone. Given that the average American spends ~50 minutes driving per day 1 , that would equate to about 18 GB captured per person, per month. This could cause significant issues for user data caps and battery life. Additional tuning would be needed to determine if a lower resolution, lower frequency option for data capture would be acceptable. Alternatively, looking for ways to run the models on the user device (such as Tensorflow.js) would likely reduce data use (though increase battery use). Source - Link \u21a9","title":"Architecture"},{"location":"Deployment/#architecture","text":"The majority of this solution was deployed using AWS Sagemaker and AWS Rekognition along with standard AWS services such as Lambda, Step Functions, DynamoDB.","title":"Architecture"},{"location":"Deployment/#synchronous_trip_data_capture","text":"The first half of this system focuses on the capture of driver data. A small website was developed ( source code ) that a user can access with their Smartphone. When the user starts a \"trip\", images and GPS data are automatically captured every 5 seconds. This data is sent to an API which saves the photo and associated metadata for future processing. Visualized below are a series of photos captured through this system at 5 second intervals and 480p resolution.","title":"Synchronous Trip Data Capture"},{"location":"Deployment/#asynchronous_trip_summary_pipeline","text":"As a batch process, trip data is processed through a machine vision pipeline. When triggered, the batch process hosts the 3 sagemaker models discussed in the \"Models\" section of this book on Sagemaker Endpoints. The pipeline then loops through each unique trip id, and grabs all of the associated image URLs from a DynamoDB database. Each image is then processed through a Lambda which invokes the Sagemaker Endpoints as well as the Rekognition detect_labels API. The model detections are then stored in another DynamoDB database for downstream use. Finally, the sagemaker endpoints are shut down once all trips have been processed. The backend source code can be found here","title":"Asynchronous Trip Summary Pipeline"},{"location":"Deployment/#future_enhancements","text":"","title":"Future Enhancements"},{"location":"Deployment/#scale","text":"The system built for this project works well for small-medium scale. However, for a major insurance company to use it, there would likely need to be some hardening done on the batch pipeline. One service to look into would be AWS Kinesis which may provide better scalability when processing potentially millions of images daily.","title":"Scale"},{"location":"Deployment/#security","text":"The initial system does not have user identity or user authentication implemented. Implementing a service such as AWS Cognito would be needed in order to make this production grade.","title":"Security"},{"location":"Deployment/#user_data_limits","text":"This system sends one 480p image every 5 seconds from a user's smartphone. Given that the average American spends ~50 minutes driving per day 1 , that would equate to about 18 GB captured per person, per month. This could cause significant issues for user data caps and battery life. Additional tuning would be needed to determine if a lower resolution, lower frequency option for data capture would be acceptable. Alternatively, looking for ways to run the models on the user device (such as Tensorflow.js) would likely reduce data use (though increase battery use). Source - Link \u21a9","title":"User Data Limits"},{"location":"Future-Research/","text":"Future Research \u00b6 Additional Models \u00b6 This section captures ideas for additional machine vision models that could be deployed in parallel with the existing system. The architecture for this system was built to scale indefinitely as the number of models increases, so adding an additional model should be fairly simple other than the time to train it. Driver Attention \u00b6 One area of interest would be looking for ways to predict what the driver is attending to at any given moment, or identify cases where the driver's attention must be split between many different objects in front of them. A model could be built based upon eye-tracking data from vehicle drivers to predict these situations of split attention. Theoretically, situations where a driver has split attention should be higher risk, and if someone is commonly needing to split attention, calling this to their attention may help mitigate that risk. See Predicting Driver Attention in Critical Situations for motivating examples and data. Major Road Hazard \u00b6 On the road there are many potential hazards that could raise the risk of driving (existing crashes, debris on the road, flooded roadways, etc...). A model could be built to detect these situations. If someone frequently drives in areas with these types of conditions, that is likely something that would raise their risk as a driver. Alternatively, an insurance company could implement a model that detects these types of risky events, and diverts drivers to other routes that avoid the risky situation. See Detecting Unsigned Physical Road Incidents from Driver-View Images for motivating examples and data. Multi-Image Analysis \u00b6 One key area of improvement would be adding the ability to consider multiple images together. While the current solution only considers each image in isolation (i.e. \"Is there a stop light in my view?\"), the would be the potential to consider across multiple images and make more meaningful detections (i.e. \"Am I approaching a red stop light quickly?\") Vehicle Approach Speed \u00b6 Traditional telematics systems treat \"hard braking events\" very harshly. The general logic on this being that the hard braking events are indicative of near-miss crashes. While on that specific day you may have been lucky and you stopped soon enough, on a future day you may not be. Someone who commonly has these hard braking events likely won't continue being lucky forever! A model could be built that gives additional context behind these events. Looking between a few frames right before a braking event, it would be possible to detect the object that caused the braking event, and calculate the approach speed of that object or how much notice the driver had about the object before they started to brake. Any of that data would give more or less weight to the fact that the braking event occurred. Stop Light Response \u00b6 As a driver, it is very common to be driving up to a green stop light, and have it turn yellow. In that moment you decide either to hit your brake or drive through the yellow light. A machine vision system could look at when a light turns yellow and determine how long it took for the driver to react as well as determine how appropriate their reaction (stop or go) was. As with all of these other ideas, this data could be used to determine how risky the driver is, or to give them feedback on how to be a safer driver in the future. Model Applicability to Insurance Pricing \u00b6 All of the research discussed in this paper effectively boils down to a form of feature creation. On its own, knowing \"number of cars visible in the frame\" or \"it is currently raining\" are not particularly useful points of information for an insurance company. What is useful is using that data in a downstream system to make determinations about the riskiness of an individual driver. A large branch of additional research that could be explored would be combining the features created through all of these machine vision models with a downstream target such as \"amount of insurance dollars paid in a subsequent 1 year period\". Such a model could be used by an insurance company to determine the appropriate discount someone deserves based upon how high/low risk they appear to be.","title":"Future Research"},{"location":"Future-Research/#future_research","text":"","title":"Future Research"},{"location":"Future-Research/#additional_models","text":"This section captures ideas for additional machine vision models that could be deployed in parallel with the existing system. The architecture for this system was built to scale indefinitely as the number of models increases, so adding an additional model should be fairly simple other than the time to train it.","title":"Additional Models"},{"location":"Future-Research/#driver_attention","text":"One area of interest would be looking for ways to predict what the driver is attending to at any given moment, or identify cases where the driver's attention must be split between many different objects in front of them. A model could be built based upon eye-tracking data from vehicle drivers to predict these situations of split attention. Theoretically, situations where a driver has split attention should be higher risk, and if someone is commonly needing to split attention, calling this to their attention may help mitigate that risk. See Predicting Driver Attention in Critical Situations for motivating examples and data.","title":"Driver Attention"},{"location":"Future-Research/#major_road_hazard","text":"On the road there are many potential hazards that could raise the risk of driving (existing crashes, debris on the road, flooded roadways, etc...). A model could be built to detect these situations. If someone frequently drives in areas with these types of conditions, that is likely something that would raise their risk as a driver. Alternatively, an insurance company could implement a model that detects these types of risky events, and diverts drivers to other routes that avoid the risky situation. See Detecting Unsigned Physical Road Incidents from Driver-View Images for motivating examples and data.","title":"Major Road Hazard"},{"location":"Future-Research/#multi-image_analysis","text":"One key area of improvement would be adding the ability to consider multiple images together. While the current solution only considers each image in isolation (i.e. \"Is there a stop light in my view?\"), the would be the potential to consider across multiple images and make more meaningful detections (i.e. \"Am I approaching a red stop light quickly?\")","title":"Multi-Image Analysis"},{"location":"Future-Research/#vehicle_approach_speed","text":"Traditional telematics systems treat \"hard braking events\" very harshly. The general logic on this being that the hard braking events are indicative of near-miss crashes. While on that specific day you may have been lucky and you stopped soon enough, on a future day you may not be. Someone who commonly has these hard braking events likely won't continue being lucky forever! A model could be built that gives additional context behind these events. Looking between a few frames right before a braking event, it would be possible to detect the object that caused the braking event, and calculate the approach speed of that object or how much notice the driver had about the object before they started to brake. Any of that data would give more or less weight to the fact that the braking event occurred.","title":"Vehicle Approach Speed"},{"location":"Future-Research/#stop_light_response","text":"As a driver, it is very common to be driving up to a green stop light, and have it turn yellow. In that moment you decide either to hit your brake or drive through the yellow light. A machine vision system could look at when a light turns yellow and determine how long it took for the driver to react as well as determine how appropriate their reaction (stop or go) was. As with all of these other ideas, this data could be used to determine how risky the driver is, or to give them feedback on how to be a safer driver in the future.","title":"Stop Light Response"},{"location":"Future-Research/#model_applicability_to_insurance_pricing","text":"All of the research discussed in this paper effectively boils down to a form of feature creation. On its own, knowing \"number of cars visible in the frame\" or \"it is currently raining\" are not particularly useful points of information for an insurance company. What is useful is using that data in a downstream system to make determinations about the riskiness of an individual driver. A large branch of additional research that could be explored would be combining the features created through all of these machine vision models with a downstream target such as \"amount of insurance dollars paid in a subsequent 1 year period\". Such a model could be used by an insurance company to determine the appropriate discount someone deserves based upon how high/low risk they appear to be.","title":"Model Applicability to Insurance Pricing"},{"location":"Road-Test/","text":"Road Test \u00b6 To finish this project, a \"road test\" was conducted by accessing the front-end website on a smart phone and driving around Bloomington, IL for approximately 30 minutes. Trip Path \u00b6 The trip went through a variety of different portions of Bloomington. This includes single family houses, dense historic residential areas, the downtown area, a few major roads, and a brief trip on the highway. The trip was conducted approximately an hour before sunset on a day with light cloud coverage. Trip Analysis \u00b6 Weather Model Performance \u00b6 The model appeared to do a fairly good job at detecting the clear/partially cloudy weather throughout the trip. Notably, it seems to fail in very sunny photos where the color of the sky gets washed out. Time Series \u00b6 Below shows a time series of the weather model's output throughout the trip. The model clearly lost confidence when entering a downtown region of the city where the skyline blocked its view of the sun. Additionally, the model's confidence changed materially when the trip faced Southwards on a wide-open stretch of road. Street Category Performance \u00b6 The model seems to have a large number of false positives for the \"highway\" class. As called out on the Dataset page, the training dataset appears to have a bias towards major city driving. A theory for this overuse of the \"highway\" class would be that the model is picking up on the open space in a smaller town, and it associates open space with highways. Time of Day Performance \u00b6 Time Series \u00b6 Below shows a time series of the time of day model's output throughout the trip. The model clearly changed its prediction when entering the downtown area. This appears to be related to the tall buildings blockings it's view of the setting sun. Rekognition Performance \u00b6 The Rekognition portion of the system appears to do well at identifying cars, stop lights and intersections. The only pitfall noticed was the system does not identify objects far in the distance. While this may be what is desired (a car 500 feet in front of you likely doesn\u2019t affect your driving), the inability to control this falloff was a disappointment. Intersection Map \u00b6 This map shows the intersections observed during the road test. Generally the system seems to have done well at identifying major intersections with stop lights, but often misses 3-way or uncontrolled intersections. Additionally, there appear to be multiple detections for the same intersection while approaching. A system would need to be created to prevent this in the future.","title":"Road Test"},{"location":"Road-Test/#road_test","text":"To finish this project, a \"road test\" was conducted by accessing the front-end website on a smart phone and driving around Bloomington, IL for approximately 30 minutes.","title":"Road Test"},{"location":"Road-Test/#trip_path","text":"The trip went through a variety of different portions of Bloomington. This includes single family houses, dense historic residential areas, the downtown area, a few major roads, and a brief trip on the highway. The trip was conducted approximately an hour before sunset on a day with light cloud coverage.","title":"Trip Path"},{"location":"Road-Test/#trip_analysis","text":"","title":"Trip Analysis"},{"location":"Road-Test/#weather_model_performance","text":"The model appeared to do a fairly good job at detecting the clear/partially cloudy weather throughout the trip. Notably, it seems to fail in very sunny photos where the color of the sky gets washed out.","title":"Weather Model Performance"},{"location":"Road-Test/#time_series","text":"Below shows a time series of the weather model's output throughout the trip. The model clearly lost confidence when entering a downtown region of the city where the skyline blocked its view of the sun. Additionally, the model's confidence changed materially when the trip faced Southwards on a wide-open stretch of road.","title":"Time Series"},{"location":"Road-Test/#street_category_performance","text":"The model seems to have a large number of false positives for the \"highway\" class. As called out on the Dataset page, the training dataset appears to have a bias towards major city driving. A theory for this overuse of the \"highway\" class would be that the model is picking up on the open space in a smaller town, and it associates open space with highways.","title":"Street Category Performance"},{"location":"Road-Test/#time_of_day_performance","text":"","title":"Time of Day Performance"},{"location":"Road-Test/#time_series_1","text":"Below shows a time series of the time of day model's output throughout the trip. The model clearly changed its prediction when entering the downtown area. This appears to be related to the tall buildings blockings it's view of the setting sun.","title":"Time Series"},{"location":"Road-Test/#rekognition_performance","text":"The Rekognition portion of the system appears to do well at identifying cars, stop lights and intersections. The only pitfall noticed was the system does not identify objects far in the distance. While this may be what is desired (a car 500 feet in front of you likely doesn\u2019t affect your driving), the inability to control this falloff was a disappointment.","title":"Rekognition Performance"},{"location":"Road-Test/#intersection_map","text":"This map shows the intersections observed during the road test. Generally the system seems to have done well at identifying major intersections with stop lights, but often misses 3-way or uncontrolled intersections. Additionally, there appear to be multiple detections for the same intersection while approaching. A system would need to be created to prevent this in the future.","title":"Intersection Map"},{"location":"Sources/","text":"Sources \u00b6 Data Sources \u00b6 BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning Model Architecture \u00b6 ResNet Motivating Research \u00b6 Usage-Based Insurance and Vehicle Telematics: Insurance Market and Regulatory Implications Predicting Driver Attention in Critical Situations Detecting Unsigned Physical Road Incidents from Driver-View Images AWS Services Documentation \u00b6 AWS Rekognition User Documentation AWS Sagemaker User Documentation","title":"Sources"},{"location":"Sources/#sources","text":"","title":"Sources"},{"location":"Sources/#data_sources","text":"BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning","title":"Data Sources"},{"location":"Sources/#model_architecture","text":"ResNet","title":"Model Architecture"},{"location":"Sources/#motivating_research","text":"Usage-Based Insurance and Vehicle Telematics: Insurance Market and Regulatory Implications Predicting Driver Attention in Critical Situations Detecting Unsigned Physical Road Incidents from Driver-View Images","title":"Motivating Research"},{"location":"Sources/#aws_services_documentation","text":"AWS Rekognition User Documentation AWS Sagemaker User Documentation","title":"AWS Services Documentation"},{"location":"Telematics-Context/","text":"Telematics Context \u00b6 Vehicle Telematics and the concept of \"Usage Based Insurance\" have become commonplace in the US Auto Insurance Industry over the past decade 1 2 3 . The general idea with these programs is that policyholders agree to put a device in their car that monitors their operation of the vehicle, and in return they generally 4 receive a reduction in premium. This is thought to be mutually beneficial because the insured has more agency in controlling the premium they are charged, and the insurer has more certainty around the risk they are assuming for an individual policy. The key part of these systems is the device which captures driver behavior. Traditionally, a freestanding/autonomous device was placed in the vehicle with sensors such as a GPS, an accelerometer and ways to communicate the data back to a central server. Over time, insurers have looked to reduce the cost of these devices. This generally comes in the form of smaller bluetooth enabled devices which connect to the policyholder's smartphone, or some insurers simply just use a smartphone app with no additional technology required. 5 These systems generally focus on data which can be captured using devices traditionally found on a smartphone (GPS, Accelerometer, Compass, etc...). With these sensors, it is possible to track events such as \"hard braking\", \"fast acceleration\", \"hard turning\", \"over the speed limit\". This data has a much more direct relationship to a policyholder's risk than some traditional rating variables such as location and credit report data which has a great appeal for public opinion. Some insurers have even promised to discontinue use of data such as credit reports 6 because they feel that telematics data is more appropriate. There is one additional device on smartphones which till now has not been broadly used for vehicle telematics: the camera. While much of the data captured through telematics has a meaningful causal relationship with insurance risk (i.e. It makes sense that hard braking events would relate to crashes), they are still somewhat of a proxy. There are totally valid reasons to have hard braking events in a car, and simply treating all of them as equal does not seem optimal. Where a camera could expand vehicle telematics is giving additional context around these more traditional events. If the accelerometer detects a hard braking event, can that be correlated with the driver waiting till the last minute to avoid rear ending someone, or was it because someone else ran a red light? Those two stories both had the potential to end in a crash, but \"fault\" is something that can only really be assessed when machine vision is incorporated into the solution. https://www.statefarm.com/insurance/auto/discounts/drive-safe-save \u21a9 https://www.progressive.com/auto/discounts/snapshot/ \u21a9 https://www.geico.com/driveeasy/getting-started/ \u21a9 Some programs such as Progressive's Snapshot Program do not insure a discount. \u21a9 https://www.casact.org/sites/default/files/2021-03/6U_NAIC_Telematics.pdf \u21a9 https://www.caranddriver.com/news/a33555840/root-auto-insurance-credit-scores-racism/ \u21a9","title":"Telematics Context"},{"location":"Telematics-Context/#telematics_context","text":"Vehicle Telematics and the concept of \"Usage Based Insurance\" have become commonplace in the US Auto Insurance Industry over the past decade 1 2 3 . The general idea with these programs is that policyholders agree to put a device in their car that monitors their operation of the vehicle, and in return they generally 4 receive a reduction in premium. This is thought to be mutually beneficial because the insured has more agency in controlling the premium they are charged, and the insurer has more certainty around the risk they are assuming for an individual policy. The key part of these systems is the device which captures driver behavior. Traditionally, a freestanding/autonomous device was placed in the vehicle with sensors such as a GPS, an accelerometer and ways to communicate the data back to a central server. Over time, insurers have looked to reduce the cost of these devices. This generally comes in the form of smaller bluetooth enabled devices which connect to the policyholder's smartphone, or some insurers simply just use a smartphone app with no additional technology required. 5 These systems generally focus on data which can be captured using devices traditionally found on a smartphone (GPS, Accelerometer, Compass, etc...). With these sensors, it is possible to track events such as \"hard braking\", \"fast acceleration\", \"hard turning\", \"over the speed limit\". This data has a much more direct relationship to a policyholder's risk than some traditional rating variables such as location and credit report data which has a great appeal for public opinion. Some insurers have even promised to discontinue use of data such as credit reports 6 because they feel that telematics data is more appropriate. There is one additional device on smartphones which till now has not been broadly used for vehicle telematics: the camera. While much of the data captured through telematics has a meaningful causal relationship with insurance risk (i.e. It makes sense that hard braking events would relate to crashes), they are still somewhat of a proxy. There are totally valid reasons to have hard braking events in a car, and simply treating all of them as equal does not seem optimal. Where a camera could expand vehicle telematics is giving additional context around these more traditional events. If the accelerometer detects a hard braking event, can that be correlated with the driver waiting till the last minute to avoid rear ending someone, or was it because someone else ran a red light? Those two stories both had the potential to end in a crash, but \"fault\" is something that can only really be assessed when machine vision is incorporated into the solution. https://www.statefarm.com/insurance/auto/discounts/drive-safe-save \u21a9 https://www.progressive.com/auto/discounts/snapshot/ \u21a9 https://www.geico.com/driveeasy/getting-started/ \u21a9 Some programs such as Progressive's Snapshot Program do not insure a discount. \u21a9 https://www.casact.org/sites/default/files/2021-03/6U_NAIC_Telematics.pdf \u21a9 https://www.caranddriver.com/news/a33555840/root-auto-insurance-credit-scores-racism/ \u21a9","title":"Telematics Context"},{"location":"Models/Road-Object-Detection/","text":"Rekognition Road Object Detection \u00b6 Context \u00b6 The AWS Rekognition API can detect many different objects and classifications from a variety of different domains. Included in the supported labels are many objects which would be applicable to road hazards or additional contextual information. The theory with this \"model\" is that one could simply count the number of labels from each class Rekognition detects, and use that count in downstream models. Below is a sampling of supported labels that were of interest (all of these labels are automatically detected at no marginal cost): Location Type Time of Day Road Constructs Vehicles Humans Bikers Animals \"Building\" \"Sunrise\" \"Intersection\" \"Transportation\" \"People\" \"Bicycle\" \"Deer\" \"Downtown\" \"Sunset\" \"Stopsign\" \"Automobile\" \"Person\" \"Bike\" \"Elk\" \"Alleyway\" \"Traffic Light\" \"Vehicle\" \"Pedestrian\" \"Mountain Bike\" \"Zebra\" \"Metropolis\" \"Road Sign\" \"Car\" \"Standing\" \"Mountain Bike\" \"Bear\" \"Office Building\" \"Taxi\" \"Walking\" \"Motorcycle\" \"Elephant\" \"Stadium\" \"Antique Car\" \"Motor Scooter\" \"Urban\" \"Jaguar Car\" \"Parking Lot\" \"Sports Car\" \"Tunnel\" \"Race Car\" \"Construction\" \"Pickup Truck\" \"Highway\" \"Suv\" \"House\" \"Van\" \"Mobile Home\" \"Tow Truck\" \"Dirt Road\" \"Truck\" \"Offroad\" \"Moving Van\" \"Forest\" \"Rv\" \"Tour Bus\" \"School Bus\" \"Trailer Truck\" \"Snowplow\" \"Tractor\" \"Streetcar\" \"Tram\" \"Trolley\" \"Cable Car\" \"Train\" \"Police Car\" \"Fire Truck\" \"Ambulance\" Model Implementation \u00b6 In order to use this API, a Lambda function simply invoked the API by passing the bytes of individual images. The entirety of its implementation was approximately 10 lines of code. For certain, the amount of power obtained through using this service was large compared to the amount of work needed. It is clear why a team/company that does not have access to data scientist resources would use this API for basic detections. Performance \u00b6 It was not possible to assess the performance of this model because their was a lack of applicable data. Certainly before this model were to be deployed in practice, a dataset would need to be collected to better understand how well it performs at this task. As discussed in the Road Test , the model appeared to perform well at vehicle detection and intersection detection, but it was not possible to test other labels. As a simple test, look back at the photo at the top of this page. Try to spot missed \"car\" and \"wheel\" labels in the image. While the model appears to do well, it certainly is not perfect. Additional research would be needed to understand where it is failing, and determine if that level of accuracy is acceptable. Future Enhancements \u00b6 The main enhancement needed would simply be to better understand the performance of rekognition. The BDD100k dataset could be used to an extent (though not all the labels are compatible). More likely what would be needed is a new dataset with labels directly applicable to what an insurance company intends to use the model for at the end. Building on the idea of more applicable labels, likely using the AWS Rekognition Custom Labels AutoML solution to build models more applicable to the intended use would be better than using the off-the-shelf Rekognition labels provided by Amazon.","title":"Rekognition Road Object Detection"},{"location":"Models/Road-Object-Detection/#rekognition_road_object_detection","text":"","title":"Rekognition Road Object Detection"},{"location":"Models/Road-Object-Detection/#context","text":"The AWS Rekognition API can detect many different objects and classifications from a variety of different domains. Included in the supported labels are many objects which would be applicable to road hazards or additional contextual information. The theory with this \"model\" is that one could simply count the number of labels from each class Rekognition detects, and use that count in downstream models. Below is a sampling of supported labels that were of interest (all of these labels are automatically detected at no marginal cost): Location Type Time of Day Road Constructs Vehicles Humans Bikers Animals \"Building\" \"Sunrise\" \"Intersection\" \"Transportation\" \"People\" \"Bicycle\" \"Deer\" \"Downtown\" \"Sunset\" \"Stopsign\" \"Automobile\" \"Person\" \"Bike\" \"Elk\" \"Alleyway\" \"Traffic Light\" \"Vehicle\" \"Pedestrian\" \"Mountain Bike\" \"Zebra\" \"Metropolis\" \"Road Sign\" \"Car\" \"Standing\" \"Mountain Bike\" \"Bear\" \"Office Building\" \"Taxi\" \"Walking\" \"Motorcycle\" \"Elephant\" \"Stadium\" \"Antique Car\" \"Motor Scooter\" \"Urban\" \"Jaguar Car\" \"Parking Lot\" \"Sports Car\" \"Tunnel\" \"Race Car\" \"Construction\" \"Pickup Truck\" \"Highway\" \"Suv\" \"House\" \"Van\" \"Mobile Home\" \"Tow Truck\" \"Dirt Road\" \"Truck\" \"Offroad\" \"Moving Van\" \"Forest\" \"Rv\" \"Tour Bus\" \"School Bus\" \"Trailer Truck\" \"Snowplow\" \"Tractor\" \"Streetcar\" \"Tram\" \"Trolley\" \"Cable Car\" \"Train\" \"Police Car\" \"Fire Truck\" \"Ambulance\"","title":"Context"},{"location":"Models/Road-Object-Detection/#model_implementation","text":"In order to use this API, a Lambda function simply invoked the API by passing the bytes of individual images. The entirety of its implementation was approximately 10 lines of code. For certain, the amount of power obtained through using this service was large compared to the amount of work needed. It is clear why a team/company that does not have access to data scientist resources would use this API for basic detections.","title":"Model Implementation"},{"location":"Models/Road-Object-Detection/#performance","text":"It was not possible to assess the performance of this model because their was a lack of applicable data. Certainly before this model were to be deployed in practice, a dataset would need to be collected to better understand how well it performs at this task. As discussed in the Road Test , the model appeared to perform well at vehicle detection and intersection detection, but it was not possible to test other labels. As a simple test, look back at the photo at the top of this page. Try to spot missed \"car\" and \"wheel\" labels in the image. While the model appears to do well, it certainly is not perfect. Additional research would be needed to understand where it is failing, and determine if that level of accuracy is acceptable.","title":"Performance"},{"location":"Models/Road-Object-Detection/#future_enhancements","text":"The main enhancement needed would simply be to better understand the performance of rekognition. The BDD100k dataset could be used to an extent (though not all the labels are compatible). More likely what would be needed is a new dataset with labels directly applicable to what an insurance company intends to use the model for at the end. Building on the idea of more applicable labels, likely using the AWS Rekognition Custom Labels AutoML solution to build models more applicable to the intended use would be better than using the off-the-shelf Rekognition labels provided by Amazon.","title":"Future Enhancements"},{"location":"Models/Street-Category/","text":"Street Category Classifier \u00b6 Purpose \u00b6 The purpose of this model is to detect the class of road currently being driven on. The motivation here being that the risks that exist on a residential road versus the risks that exist on a highway are materially different. A residential road has a higher risk of slow speed crashes often involving pedestrians whereas a highway will often involve high speed collisions between vehicles. Theoretically this data could be sourced through open source databases such as OpenStreetMap. However, these databases would reflect broad classifications for a road. Using machine vision, it would be possible to get much more granular. A road may broadly be a \"city street\", but it can weave in and out of \"residential\" for small segments. Machine vision would allow for knowing \"on ground level what does this street look like to a driver?\" The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 4 classes: 1) City Street 2) Highway 3) Residential 4) Undefined In practice, the Undefined class is almost never used in the training data. It is mostly reserved for artificial environments (tunnels, parking lots) or for sections where the exact class of road is unclear to the labeler or to the model. Data Considerations \u00b6 The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. Initially, the dataset included 7 classes of road. However, \"Gas Station\", \"Parking Lot\", and \"Tunnel\" were not well populated. For this reason, these classes were combined with the existing \"Undefined\" class. Additionally, the dataset was highly imbalanced. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count city street 43,516 25% 10,820 highway 17,379 50% 8,620 residential 8074 100% 8,074 undefined 894 100% 894 Total 69,863 40.7% 28,408 Model Architecture \u00b6 The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 1 terminating with a 4-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16 Performance \u00b6 Overall the performance of the model appears to be middle of the road with a 67% accuracy on validation. In general, the model's accuracy seems to be impacted by unclear definitions of \"residential\" vs \"city street\" in the dataset. The intended definition seems to be single-family dense neighbors (residential) versus commercial district streets (city street). However, these are fairly similar in the grand scheme of all roads that can exist. Potentially re-assessing the data collected and the labels to have a broader span of classes (i.e. dirt road, suburban residential, small town commercial) would be a better way to assess performance. Find additional fit statistics in the appendix . Future Enhancements \u00b6 The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"street category\" model was the definitions of Residential vs City Street. A better dataset for this purpose would likely have a broader class of roads (dirt road, suburban residential, small town commercial) rather than being very specific about the difference between City Residential vs City Commercial. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues. Appendix \u00b6 F1 Score \u00b6 precision recall f1-score support City Street 0.85 0.64 0.73 6112 Highway 0.52 0.86 0.65 2499 Residential 0.58 0.51 0.54 1253 Undefined 0.32 0.38 0.35 136 accuracy 0.67 10000 macro avg 0.57 0.60 0.57 10000 weighted avg 0.73 0.67 0.68 10000 Confusion Matrix \u00b6 Model Interpretation \u00b6 Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. City Street Highway Residential Undefined ResNet Architecture Paper \u21a9","title":"Street Category Classifier"},{"location":"Models/Street-Category/#street_category_classifier","text":"","title":"Street Category Classifier"},{"location":"Models/Street-Category/#purpose","text":"The purpose of this model is to detect the class of road currently being driven on. The motivation here being that the risks that exist on a residential road versus the risks that exist on a highway are materially different. A residential road has a higher risk of slow speed crashes often involving pedestrians whereas a highway will often involve high speed collisions between vehicles. Theoretically this data could be sourced through open source databases such as OpenStreetMap. However, these databases would reflect broad classifications for a road. Using machine vision, it would be possible to get much more granular. A road may broadly be a \"city street\", but it can weave in and out of \"residential\" for small segments. Machine vision would allow for knowing \"on ground level what does this street look like to a driver?\" The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 4 classes: 1) City Street 2) Highway 3) Residential 4) Undefined In practice, the Undefined class is almost never used in the training data. It is mostly reserved for artificial environments (tunnels, parking lots) or for sections where the exact class of road is unclear to the labeler or to the model.","title":"Purpose"},{"location":"Models/Street-Category/#data_considerations","text":"The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. Initially, the dataset included 7 classes of road. However, \"Gas Station\", \"Parking Lot\", and \"Tunnel\" were not well populated. For this reason, these classes were combined with the existing \"Undefined\" class. Additionally, the dataset was highly imbalanced. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count city street 43,516 25% 10,820 highway 17,379 50% 8,620 residential 8074 100% 8,074 undefined 894 100% 894 Total 69,863 40.7% 28,408","title":"Data Considerations"},{"location":"Models/Street-Category/#model_architecture","text":"The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 1 terminating with a 4-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16","title":"Model Architecture"},{"location":"Models/Street-Category/#performance","text":"Overall the performance of the model appears to be middle of the road with a 67% accuracy on validation. In general, the model's accuracy seems to be impacted by unclear definitions of \"residential\" vs \"city street\" in the dataset. The intended definition seems to be single-family dense neighbors (residential) versus commercial district streets (city street). However, these are fairly similar in the grand scheme of all roads that can exist. Potentially re-assessing the data collected and the labels to have a broader span of classes (i.e. dirt road, suburban residential, small town commercial) would be a better way to assess performance. Find additional fit statistics in the appendix .","title":"Performance"},{"location":"Models/Street-Category/#future_enhancements","text":"The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"street category\" model was the definitions of Residential vs City Street. A better dataset for this purpose would likely have a broader class of roads (dirt road, suburban residential, small town commercial) rather than being very specific about the difference between City Residential vs City Commercial. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues.","title":"Future Enhancements"},{"location":"Models/Street-Category/#appendix","text":"","title":"Appendix"},{"location":"Models/Street-Category/#f1_score","text":"precision recall f1-score support City Street 0.85 0.64 0.73 6112 Highway 0.52 0.86 0.65 2499 Residential 0.58 0.51 0.54 1253 Undefined 0.32 0.38 0.35 136 accuracy 0.67 10000 macro avg 0.57 0.60 0.57 10000 weighted avg 0.73 0.67 0.68 10000","title":"F1 Score"},{"location":"Models/Street-Category/#confusion_matrix","text":"","title":"Confusion Matrix"},{"location":"Models/Street-Category/#model_interpretation","text":"Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. City Street Highway Residential Undefined ResNet Architecture Paper \u21a9","title":"Model Interpretation"},{"location":"Models/Time-Of-Day-Detection/","text":"Time of Day Classifier \u00b6 Purpose \u00b6 The purpose of this model is to detect the light level (a.k.a. time of day) that the driver has vision of. The motivation here being that dawn/dusk is known to be the most risky time of day to drive. 1 Potentially time of day could be approximated by just using a clock, but the relationship is not that simple. Light Level ~ Clock Time + Timezone + Geographic Position + Vehicle Heading + etc... Using machine vision to visually assess \"is the sun rising/setting in front of me right now?\" is seemingly simpler than trying to think through all of that complexity. The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 4 classes: 1) Day 2) Dawn/Dust 3) Night 4) Undefined In practice, the Undefined class is almost never used in the training data. It is mostly reserved for totally artificial environments such as a tunnel where no daylight is visible in the photo. Data Considerations \u00b6 The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. The dataset was highly imbalanced initially. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count Daytime 36,728 15% 5,574 Dawn/Dusk 5,027 100% 5,027 Night 27,971 20% 5,672 undefined 137 100% 137 Total 69,863 23.5% 16,410 Model Architecture \u00b6 The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 2 terminating with a 4-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16 Performance \u00b6 Overall the performance of the model appears to be fairly good with a 92% accuracy on validation. However, the model appears to struggle with the \"Dawn/Dusk\" class (often labeling it as \"daytime\" instead). In part, I assume this is due to the ambiguity of how \"Dawn/Dusk\" was defined during labeling. Find additional fit statistics in the appendix . Future Enhancements \u00b6 The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"time of day\" model was the definitions of daytime/dusk/night were inconsistent between photos. A better dataset for this purpose would likely have a numeric target like \"hours till sunset\" which is more objective. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues. Appendix \u00b6 F1 Score \u00b6 precision recall f1-score support Daytime 0.93 0.94 0.94 5258 Dawn/Dusk 0.57 0.53 0.55 778 Night 0.98 0.98 0.98 3929 Undefined 0.74 0.57 0.65 35 accuracy 0.92 10000 macro avg 0.80 0.75 0.78 10000 weighted avg 0.92 0.92 0.92 10000 Confusion Matrix \u00b6 Model Interpretation \u00b6 Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. Day Dawn/Dusk Night Undefined AARP recommends older drivers avoid driving during dusk \u21a9 ResNet Architecture Paper \u21a9","title":"Time of Day Classifier"},{"location":"Models/Time-Of-Day-Detection/#time_of_day_classifier","text":"","title":"Time of Day Classifier"},{"location":"Models/Time-Of-Day-Detection/#purpose","text":"The purpose of this model is to detect the light level (a.k.a. time of day) that the driver has vision of. The motivation here being that dawn/dusk is known to be the most risky time of day to drive. 1 Potentially time of day could be approximated by just using a clock, but the relationship is not that simple. Light Level ~ Clock Time + Timezone + Geographic Position + Vehicle Heading + etc... Using machine vision to visually assess \"is the sun rising/setting in front of me right now?\" is seemingly simpler than trying to think through all of that complexity. The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 4 classes: 1) Day 2) Dawn/Dust 3) Night 4) Undefined In practice, the Undefined class is almost never used in the training data. It is mostly reserved for totally artificial environments such as a tunnel where no daylight is visible in the photo.","title":"Purpose"},{"location":"Models/Time-Of-Day-Detection/#data_considerations","text":"The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. The dataset was highly imbalanced initially. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count Daytime 36,728 15% 5,574 Dawn/Dusk 5,027 100% 5,027 Night 27,971 20% 5,672 undefined 137 100% 137 Total 69,863 23.5% 16,410","title":"Data Considerations"},{"location":"Models/Time-Of-Day-Detection/#model_architecture","text":"The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 2 terminating with a 4-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16","title":"Model Architecture"},{"location":"Models/Time-Of-Day-Detection/#performance","text":"Overall the performance of the model appears to be fairly good with a 92% accuracy on validation. However, the model appears to struggle with the \"Dawn/Dusk\" class (often labeling it as \"daytime\" instead). In part, I assume this is due to the ambiguity of how \"Dawn/Dusk\" was defined during labeling. Find additional fit statistics in the appendix .","title":"Performance"},{"location":"Models/Time-Of-Day-Detection/#future_enhancements","text":"The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"time of day\" model was the definitions of daytime/dusk/night were inconsistent between photos. A better dataset for this purpose would likely have a numeric target like \"hours till sunset\" which is more objective. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues.","title":"Future Enhancements"},{"location":"Models/Time-Of-Day-Detection/#appendix","text":"","title":"Appendix"},{"location":"Models/Time-Of-Day-Detection/#f1_score","text":"precision recall f1-score support Daytime 0.93 0.94 0.94 5258 Dawn/Dusk 0.57 0.53 0.55 778 Night 0.98 0.98 0.98 3929 Undefined 0.74 0.57 0.65 35 accuracy 0.92 10000 macro avg 0.80 0.75 0.78 10000 weighted avg 0.92 0.92 0.92 10000","title":"F1 Score"},{"location":"Models/Time-Of-Day-Detection/#confusion_matrix","text":"","title":"Confusion Matrix"},{"location":"Models/Time-Of-Day-Detection/#model_interpretation","text":"Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. Day Dawn/Dusk Night Undefined AARP recommends older drivers avoid driving during dusk \u21a9 ResNet Architecture Paper \u21a9","title":"Model Interpretation"},{"location":"Models/Weather-Detection/","text":"Weather Classifier \u00b6 Purpose \u00b6 The purpose of this model is to detect the weather condition that the driver has vision of. The motivation here being that rain/snow are risky conditions to drive in. Potentially other sources of weather data (i.e. such as NOAA) could be used to source this information. However, a road trip may span multiple weather conditions, and the proposed method would allow for more granular details about what the weather looked like from the driver's view at any given moment. The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 6 classes: 1) Clear 2) Partly Cloudy 3) Overcast 4) Rainy 5) Snowy 6) Undefined In practice, the Undefined class is mostly used for night-time conditions where the sky is not visible, and road conditions do not allow for the assessment of the weather. Data Considerations \u00b6 The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. Initially, the dataset included 7 classes of weather. However, \"Foggy\" was not well populated. For this reason, that class was combined with the existing \"Undefined\" class. Additionally, the dataset was highly imbalanced. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count Clear 37,344 15% 5,518 Partly Cloudy 4,881 100% 4,881 Overcast 8,770 75% 6,554 Rainy 5,070 100% 5,070 Snowy 5,549 100% 5,549 Undefined 8,249 75% 6,202 Total 69,863 48.3% 33,774 Model Architecture \u00b6 The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 1 terminating with a 6-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16 Performance \u00b6 Overall the performance of the model appears to be middle of the road with a 74% accuracy on validation. In general, the model's accuracy seems to be impacted by the fuzzy line between classes: at what point does a cloudy sky become \"rainy\"? In reviewing mis-classification examples, commonly the ground-truth label was unclear. For example, an image of clear weather sky, but a small amount of snow on the side of the road was labeled \"snowy\". While that label is not objectively wrong, I think reasonable minds would agree with the models label of \"clear\" as well. Find additional fit statistics in the appendix . Future Enhancements \u00b6 The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"weather\" model was the lack of focus on dangerous weather conditions. The difference between \"Clear\" and \"Partly Cloudy\" is almost surely not meaningful for the intended downstream use of this model. Ideally, there would be more classes of adverse weather (i.e. foggy, flooded, roads fully covered in snow), and less granularity of the \"safe\" classes. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues. Appendix \u00b6 F1 Score \u00b6 precision recall f1-score support Clear 0.92 0.79 0.85 5346 Partly Cloudy 0.46 0.74 0.57 738 Overcast 0.58 0.64 0.61 1239 Rainy 0.66 0.66 0.66 738 Snowy 0.78 0.59 0.67 769 Undefined 0.58 0.75 0.65 1170 accuracy 0.74 10000 macro avg 0.66 0.70 0.67 10000 weighted avg 0.77 0.74 0.75 10000 Confusion Matrix \u00b6 Model Interpretation \u00b6 Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. Snowy Rainy Overcast Partly Cloudy Clear Day Undefined ResNet Architecture Paper \u21a9","title":"Weather Classifier"},{"location":"Models/Weather-Detection/#weather_classifier","text":"","title":"Weather Classifier"},{"location":"Models/Weather-Detection/#purpose","text":"The purpose of this model is to detect the weather condition that the driver has vision of. The motivation here being that rain/snow are risky conditions to drive in. Potentially other sources of weather data (i.e. such as NOAA) could be used to source this information. However, a road trip may span multiple weather conditions, and the proposed method would allow for more granular details about what the weather looked like from the driver's view at any given moment. The proposed model takes a single 3 x 224 x 224 image as input, and outputs probability of 6 classes: 1) Clear 2) Partly Cloudy 3) Overcast 4) Rainy 5) Snowy 6) Undefined In practice, the Undefined class is mostly used for night-time conditions where the sky is not visible, and road conditions do not allow for the assessment of the weather.","title":"Purpose"},{"location":"Models/Weather-Detection/#data_considerations","text":"The model was trained using the BDD100k dataset as described previously . This dataset has approximately 70,000 training images and 10,000 validation images. Images were resized down to 224 x 224 pixels in order to align with the Sagemaker Image Classification container. Initially, the dataset included 7 classes of weather. However, \"Foggy\" was not well populated. For this reason, that class was combined with the existing \"Undefined\" class. Additionally, the dataset was highly imbalanced. As shown below, data was down-sampled for training. Validation statistics reported further down are based upon the original validation dataset. Level Original Count Down Sample Rate Final Count Clear 37,344 15% 5,518 Partly Cloudy 4,881 100% 4,881 Overcast 8,770 75% 6,554 Rainy 5,070 100% 5,070 Snowy 5,549 100% 5,549 Undefined 8,249 75% 6,202 Total 69,863 48.3% 33,774","title":"Data Considerations"},{"location":"Models/Weather-Detection/#model_architecture","text":"The model was trained using the AWS Sagemaker Image Classification container. The model is trained using MXNet, it is a convolutional neural network. Beyond that, the AWS user documentation unfortunately does not give a ton of details on the architecture built behind the scenes. A raw visualization of the architecture exported from Sagemaker can be found here . It appears to match the ResNet architecture 1 terminating with a 6-node classification head. Below are the key hyperparameters that were selected: Hyperparameter Value Notes Epochs 6 Pretrained Weights 1 AWS provides weights pretrained on the ImageNet with 11,000 categories Image Size 3 x 224 x 224 Pretrained weights are only supported at this image size Layers 18 The minimum supported layer count. The model did show elements of overfitting even at this restricted layer count Optimizer Adam Learning Rate 0.001 Mini Batch Size 16","title":"Model Architecture"},{"location":"Models/Weather-Detection/#performance","text":"Overall the performance of the model appears to be middle of the road with a 74% accuracy on validation. In general, the model's accuracy seems to be impacted by the fuzzy line between classes: at what point does a cloudy sky become \"rainy\"? In reviewing mis-classification examples, commonly the ground-truth label was unclear. For example, an image of clear weather sky, but a small amount of snow on the side of the road was labeled \"snowy\". While that label is not objectively wrong, I think reasonable minds would agree with the models label of \"clear\" as well. Find additional fit statistics in the appendix .","title":"Performance"},{"location":"Models/Weather-Detection/#future_enhancements","text":"The model trained on the BDD100k dataset does not exactly meet the intended use of that data. Teaching autonomous vehicles to drive isn't directly in line with my intended use (identifying risks to a human driver). While it was good enough for a school project, future research should look to collect a different dataset more in line with this use case. A big gap applicable to this \"weather\" model was the lack of focus on dangerous weather conditions. The difference between \"Clear\" and \"Partly Cloudy\" is almost surely not meaningful for the intended downstream use of this model. Ideally, there would be more classes of adverse weather (i.e. foggy, flooded, roads fully covered in snow), and less granularity of the \"safe\" classes. Additionally, looking to rebuild the model in a different tool stack would likely be good. The AWS Sagemaker Image Classification container was used as a learning opportunity, but the lack of control I had over the model was limiting. The model was prone to overfitting, and that sagemaker container does not give many options for a data scientist to mitigate those issues.","title":"Future Enhancements"},{"location":"Models/Weather-Detection/#appendix","text":"","title":"Appendix"},{"location":"Models/Weather-Detection/#f1_score","text":"precision recall f1-score support Clear 0.92 0.79 0.85 5346 Partly Cloudy 0.46 0.74 0.57 738 Overcast 0.58 0.64 0.61 1239 Rainy 0.66 0.66 0.66 738 Snowy 0.78 0.59 0.67 769 Undefined 0.58 0.75 0.65 1170 accuracy 0.74 10000 macro avg 0.66 0.70 0.67 10000 weighted avg 0.77 0.74 0.75 10000","title":"F1 Score"},{"location":"Models/Weather-Detection/#confusion_matrix","text":"","title":"Confusion Matrix"},{"location":"Models/Weather-Detection/#model_interpretation","text":"Below is an example image from each class where the model correctly labeled the image. When reading these images, a blue region means that it contributed to the confidence, and a red region means it detracted from the confidence. Snowy Rainy Overcast Partly Cloudy Clear Day Undefined ResNet Architecture Paper \u21a9","title":"Model Interpretation"}]}